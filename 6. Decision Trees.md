# Decision Tree
Its a Non-linear and non-continuous model

CART (classification and Regression Tree) is composed of 2 trees:
1. Classification Tree
2. Regression Tree

<img src="https://user-images.githubusercontent.com/54764108/164891383-bc7571a2-974f-4ee4-a43e-8953ea06de02.png" width="500"> <img src="https://user-images.githubusercontent.com/54764108/164891433-5171af47-7b65-4db1-8bee-9f1f71c228c2.png" width="500">

**Fig:** Splitting the plane of independent variables into different regions and calculate avg dependent y variable results.Then making the decision tree according to that

### Importing neccessary libraries for the decision tree model and fitting

```python 
from sklearn.tree import DecisionTreeRegressor
regressor = DecisionTreeRegressor(random_state = 0)
regressor.fit(X,y)
```

### Plotting:
```python
plt.scatter(X, y, color = 'red')
plt.plot(X, regressor.predict(X), color = 'blue')
plt.title('Truth or Bluff (Decision tree Regression Model)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()
```

### TRAP
This is not the actual expected result as the plot is formed for 10 distinct levels/ 10 distict points of the independent variable <br>
So while the interval lines should have been constant/ horizontal to x-axis due to the average of those intervals,<br> there were no intervals whatsoever. <br> Rather, there were just 10 points. and those were just 'connected'

### Solution:
Therfore, we need to check for higher resolution/precise grids of independent variables so that we define intervals of X. 

```python
X_grid = np.arange(min(X), max(X), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('Truth or Bluff (Decision tree Regression Model)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()
```
![image](https://user-images.githubusercontent.com/54764108/164912672-d5c06c09-e50b-4232-ae4c-bba1e05dbe46.png)

### Making the grids more precise:
As the above plot includes "almost - vertical lines", it doesn't quite represent the concept of "non-continuity" <br>
so we need to increase the resolution to make them more vertical.
```python
X_grid = np.arange(min(X), max(X), 0.01)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('Truth or Bluff (Decision tree Regression Model)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()
```

![image](https://user-images.githubusercontent.com/54764108/164912689-7ee19990-ce25-4a4b-b82d-6656667a1b2b.png)

### Predicting data
```python
y_pred =  regressor.predict([[6.5]])
```

> output:  150000
