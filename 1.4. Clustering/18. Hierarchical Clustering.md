# Hierarchical Clustering
<img src = https://user-images.githubusercontent.com/54764108/167429990-814a1da1-ae60-48c2-a676-7221073bfd18.png width = "700">

## Types

There are 2 types of hierarchiacal clustering:
1. Agglomerative (Bottom up Approach)
2. Divisive


## Agglomerative (Bottom up Approach)
Steps:
* **Step 1 :** Make each data point a single-point cluster => forms N clusters
* **Step 2 :** Take 2 closest data points and make them one cluster => forms N-1 clusters
* **Step 3 :** Take 2 closest clusters and make them one cluster => forms N-2 clusters
* **Step 4 :** Repeat step 3 until we get only one cluster 

### Distance between two clusters:
The process of determining the distance of two clusters significantly impact the output of our model. <br>
Option 1 : closest points
Option 2 : furthest points
Option 3 : average distance
Option 4 : distance between centroids

### Purpose
* What Hierarchical Clustering does that it maintains a "memory" of how we went through the process
* And that memory is stored in "Dendrograms"

## Dendrograms
<img src = https://user-images.githubusercontent.com/54764108/167434250-f86978e1-942c-4164-aacf-74a88552df9c.png width = "700">

* At 2nd step of our algorithm we choose two closest data points and make them single clusters. and we store that information in our dandrogram. 
* height at which that green horizontal line resides determines the euclidean distance between the 2 clusters/points 
* At some step, groups of (p2 and p3) are colser to p1 and so we combine them into one cluster and show them in the dandrogram as follows:
<img src = https://user-images.githubusercontent.com/54764108/167435559-ae18f247-23d5-483d-89c4-118e844e2f8a.png width = "700">

* Final step when model is ready 
<img src = https://user-images.githubusercontent.com/54764108/167435683-1ce3b279-1e4a-4381-bb9c-55faa300b2c1.png width = "700">

## Using Dandrograms
From the above image, <br>
* We set a threshold in the distances, meaning that we are not allowing some merging of clusters if their dissimilarities go beyond that threshold. <br>
* That cuts our dendrogram to declare two clusters.
* The number of vertical lines that cross the thresold line = number of total clusters <br>
<img src = https://user-images.githubusercontent.com/54764108/167437062-a787acdc-4405-416b-a121-db08ff6194ae.png width = "450"> <img src = https://user-images.githubusercontent.com/54764108/167437505-ee2e2d6e-7445-4303-a288-e99eb94c9e72.png width = "450">

### Optimal # of clusters
* SO we try to find the highest vertical distance "that doesnt cross any horizontal lines(even projected/extended crossings)" 
* And make thresold that crosses that vertical lines

<img src = https://user-images.githubusercontent.com/54764108/167438921-6340fbf6-10ff-4d29-bad2-03f4a8d38d5a.png width = "700">

### Test
Based on the Dandrogram below, try to find the scatter plot and guess the clusters:
<img src = https://user-images.githubusercontent.com/54764108/167439269-13a66313-2759-4954-be64-0425901a8a78.png width = "500">


#### Answer
<img src = https://user-images.githubusercontent.com/54764108/167439473-01bddf6a-ec2b-442e-8bc7-99ab1282f121.png width = "700">


## Importing libraries

### Dendrogram
```python
import scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()
```
![image](https://user-images.githubusercontent.com/54764108/167446973-7013b2bc-2524-4902-8227-6198ebfe66c7.png)

* 'ward' method tries to minimize the variance with each cluster

### Fitting Hierarchical Clustering to the dataset
```python
from sklearn.cluster import AgglomerativeClustering
hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')
y_hc = hc.fit_predict(X)
```
### Visualizing (Similar to k-means and only applicable for 2 dimensions of features)
```python
plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Careful')
plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Standard')
plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Target')
plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'brown', label = 'Careless')
plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Sensible')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()
```
![image](https://user-images.githubusercontent.com/54764108/167447271-9c79c572-408a-4069-933e-b2ef797c6a20.png)



