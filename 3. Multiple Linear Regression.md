# Multiple Linear Regression

```
y = b0 + b1*xx1 + b2*x2 + ... bn*xn

x's -> independent var 
y   -> dependent var
```

### A caveat

Assumptions of Linear Regression
1. Linearity
2. Homoscedasticity
3. Multivariat normality
4. Independence of errors
5. Lack of multicollinearity

### Dummy Variable

![image](https://user-images.githubusercontent.com/54764108/164259457-63406031-6c40-4255-87a9-bdfd4c920e36.png)

#### trap:
![image](https://user-images.githubusercontent.com/54764108/164260206-762e3969-330e-44cb-b1e8-d6a34927c3e3.png)


### Building a Model
- All-in
- Backward Elimination
- Forward Selection
- Bidirectional Elimination
- Score comparison

### A. Backward Elimination

####  Step - 1
> select significant level to stay in the model (e.g. SL = 0.05)
####  Step - 2
> fit full model with all possible predictors
####  Step - 3
> consider the predictor with the highest p value<br>
  If P>SL go to step 4<br>
  else go to FIN
####  Step - 4
> remove the predictor
####  Step - 5
> fit model without the variable (potentially goto step 3)

#### FIN = Model is finished/ready

<br>

### B. Forward Selection

####  Step - 1
> select significant level to enter the model (e.g. SL = 0.05)
####  Step - 2
> fit all simple regression models y ~ xn select the one with the lowest P-value
####  Step - 3
> kepp this variable and fit all possible models with one extra predictor added to the one(s) you already have <br>
####  Step - 4
> consider the predictor with the lowest P-value. 
> if P < SL, go to step-3
> else go to FIN 

#### FIN = Keep the previous Model 

<br>

### C. Bidirectional Elimination

#### Step - 1
> select a significant level to enter and to stay in the model (e.g. SLENTER = 0.05, SLSTAY = 0.05)
####  Step - 2
> perform the next step of Forward elimination (new variables must have P<SLENTER to enter)
####  Step - 3
> perform all steps of backward elimination (old variables must have P<SLSTAY to stay)
####  Step - 4
> no new variables can enter and old variables can exit
#### FIN = Model is ready 
<br>


### D. All possible models

#### Step - 1
> select criterion of goodness of fot(e.g. Akaike criterion)
####  Step - 2
> construct all possible regression models: 2^n - 1 total combinations
####  Step - 3
> select one with best criterion

#### FIN = Model is ready

<br>

### Adjusting for Backward elimination (by adding columns of one's)

```python
import statsmodels.api as sm
X = np.append(arr = np.ones((50,1)).astype(int), values= X, axis = 1)
# here ((50,1)) means 50 for 50 rows of content
```
previous versions had 
``` 
statsmodel.formula.api
```

but now it should be
```
statsmodel.api
```

otherwise it would run into error

here ((50,1)) means 50 for 50 rows of content

### Building the optimal model using backward elimination

```python
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
regressor_OLS = sm.OLS(endog = y,exog = X_opt).fit()
regressor_OLS.summary()
```

observing the P-values, the x2 variable has the highest among all, so we omit this predictor and build the model again

![image](https://user-images.githubusercontent.com/54764108/164422894-be434b90-2e74-438c-90a6-031271eea73a.png)

```python
X_opt = X[:, [0, 1, 3, 4, 5]]
regressor_OLS = sm.OLS(endog = y,exog = X_opt).fit()
regressor_OLS.summary()
```
so on and so forth until we get a perfect model
```python
X_opt = X[:, [0, 3]]
regressor_OLS = sm.OLS(endog = y,exog = X_opt).fit()
regressor_OLS.summary()
```
![image](https://user-images.githubusercontent.com/54764108/164423214-406918ad-e277-4145-ae96-aee13c431942.png)
