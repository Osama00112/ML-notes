# Random Forest Classification

### Essemble Learning
Essemble learning is when we take multiple ML algorithms and combine them into a bigger ML algorithm <br>

Random forest is an essemble of a lot of decision tree algorithms. 

####  Step - 1
> Pick at random K data points from the Training set.
####  Step - 2
> Build the Decision Tree associated to these K data points.
####  Step - 3
> Choose the number Ntree of trees you want to build and repeat STEPS 1 & 2
####  Step - 4
> For a new data point, make each one of your Ntree trees predict the category to which the data points belong
> Assign the new data point to the category that wins the majority vote.

### Body Part Recognition
In motion sensor type games that connects the user movements to the machine algorithm, it detects the user movements and convert them to ML algorithm where random forest are used to classify different parts of user body. <br>

### Careful of overfitting
Decision trees usually try to fit every data point in the plot which may result in overfitting when used in random forest. <br>
varying the number of decsion trees will help us predict that overfitting if occurs.
#### with 10 trees
```python
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = "entropy", random_state = 0)
classifier.fit(X_train,y_train)
```
![image](https://user-images.githubusercontent.com/54764108/167260091-f5549213-28a5-4f64-bbe5-6d25b1e67ae3.png)

incorrect predictions = 5 + 4 = 9

```python
# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Random Forest Classifier (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

```
![image](https://user-images.githubusercontent.com/54764108/167260105-5dfc0bf7-6be6-4f14-b346-6a1ae5a837f6.png)


### Analysis
Clearly there are overfittings in the traning sets. What happens here is that, random forest predicts the test data points through 10 decsion trees and the gets a "majority vote" <br>
Hence final result is predicted. <br>
Now we need to check if this overfitting result compromises our test set results.


```python
# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Random Forest Classifier (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
```
![image](https://user-images.githubusercontent.com/54764108/167260113-f0c3c25c-1745-4126-8ea9-093265d3653c.png)

![image](https://user-images.githubusercontent.com/54764108/167260392-c7cb088c-486a-4a1f-afa5-dc41dc0f79c2.png)

this specific region is clearly created by overfitting, and it makes "no sense" for the new obsevations to be classified. <br>
so overall more realistic and suitable classifers are Kernel SVM or even naive bayes classifiers.
