# Decision Tree Classification

### Decision Tree
Its a Non-linear and non-continuous model

**CART** (classification and Regression Tree) is composed of 2 trees:
1. Classification Tree
2. Regression Tree

<img src="https://user-images.githubusercontent.com/54764108/167257937-130e824c-1e1f-4803-83ee-5c4092ed893c.png" width="500"> <img src="https://user-images.githubusercontent.com/54764108/167257889-719dcdf0-535a-457b-b2f1-30587c0c2892.png" width="500">

**Fig:** Splitting the plane of independent variables into different regions and calculate avg dependent y variable results.Then making the decision tree according to that and classify different samples and tests.

### Note
Decision trees are not algorithm based on euclidean distance. <br>
SO we donot actually need feature scaling. because its a "must do" preprocessing step when algorithms are based on euclidean distance. <br>

However, when plotting those desicion regions, the code will generate plot much faster when the features are scaled. so we keep the scaling in this case.

### Importing classifier

```python
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = "entropy", random_state = 0)
classifier.fit(X_train, y_train)
```


### Confusion Matrix
![image](https://user-images.githubusercontent.com/54764108/167258593-7ff90bc0-eee4-445e-884b-a28fd8352ad9.png)

total incorrect predictions = 3 + 6 = 9

```python
# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Desicion Tree Classifier (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
```
![image](https://user-images.githubusercontent.com/54764108/167258629-fed459d2-0321-4da7-a694-4c3c1d55709f.png)
```python
# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Desicion Tree Classifier (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
```
![image](https://user-images.githubusercontent.com/54764108/167258636-8fdacb73-5201-4fff-9e59-dabb23ee57e6.png)
