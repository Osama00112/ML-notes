# Kernel SVM

<img src = https://user-images.githubusercontent.com/54764108/165955895-257ad2a3-b096-4c8c-ab57-08352987d65a.png width = "600">

### Mapping to a higher dimension

#### (i) 1D to 2D:
Here we increase the dimention so that the data points can be linearly seperated. <br>

<img src = https://user-images.githubusercontent.com/54764108/165957617-762786f5-8f9a-42c7-a2f3-9ad661825798.png width = "600">
This is an example of data points that are not linearly seperable

Let us define a function so that the data points shifts w.r.t. a different reference point.<br>

> f = x - 5
<img src = https://user-images.githubusercontent.com/54764108/165957906-da17aac9-b169-4f5d-8bd9-0181dee37eb4.png width = "600">

Now taking square on the right side,
 > f = (x-5) <sup> 2 </sup>  <br>
 
which then makes the graph looks like this.

<img src = https://user-images.githubusercontent.com/54764108/165958404-b83b56aa-1da8-4b50-9459-2f5aeff57703.png width = "600">

Which is indeed linearly seperable.

#### (ii) 2D to 3D:
<img src = https://user-images.githubusercontent.com/54764108/165959204-8c9be9e7-ae41-43d5-9723-df420250ac22.png width = "600">

Here we defined some mapping function to map the 2D data points in 3 dimentional space. and after that we project that back to the 2D plane. <br>
As a result, we can see a circular boundary w.r.t. the hyper plane.

<img src = https://user-images.githubusercontent.com/54764108/165959653-61b457a0-6186-420b-8285-ad04906219c8.png width = "600">

Thus we find a non linear seperator

### BUT THERE IS A CATCH...
***Mapping to a higher dimensional space can be highly compute-intensive*** <br>
So, we propose another approach

### Kernel Trick

#### Gaussian 'rbf' Kernel:

<img src = https://user-images.githubusercontent.com/54764108/165961189-5f9a22f2-127f-44bf-bf7d-3458a335faa7.png width = "600">

<img src = https://user-images.githubusercontent.com/54764108/165962291-c157599e-6b04-4e1c-8955-9a55bf1e9a38.png width = "600">

The red point in the base is called the kernel. and we are calculating the distance from the "landmark". <br>

If,  || x - l<sup>i</sup> ||  →  infinity <br>
=> e <sup> - || x - l<sup>i</sup> ||<sup>2</sup> </sup>  →  0


And If,  || x - l<sup>i</sup> ||  →  0 <br>
=> e <sup> - || x - l<sup>i</sup> ||<sup>2</sup> </sup>  →  1


<img src = https://user-images.githubusercontent.com/54764108/165974843-16ce1ba4-1f0e-4c47-8dd4-1e5e1f7f7b06.png width = "600">

In this way, we define such kernel function that can project those points (in 2D) and thus use it for the linear seperation.<br>

The value of σ indicates the circumference of the seperator. <br>
if "σ" is large, it generates large circumferenced seperator <br>
if "σ" is small, it generates small circumferenced seperator <br>


More complex gaussian kernel:

<img src = https://user-images.githubusercontent.com/54764108/165976431-e44aac8c-0279-4185-9495-edccd637f64f.png width = "600">

We can create this non linear very complex decision boundary going into a higher dimensional space. It is actually projected in the original dimention because of the kernel function.

### Types of Kernel Functions:

1. Gaussian RBF kernel:

![image](https://user-images.githubusercontent.com/54764108/165977364-48c6edf6-5496-47d8-a123-d3a4765032ee.png)

<img src = https://user-images.githubusercontent.com/54764108/165978766-c46eb40c-16ab-48cb-a6c7-c395f9d67f13.png width = "300">


2. Sigmoid Kernel:

![image](https://user-images.githubusercontent.com/54764108/165977399-c3283ec6-b23c-45f9-bcdf-ff3377c36118.png)

<img src = https://user-images.githubusercontent.com/54764108/165978850-6783308b-efdf-4474-a99b-6f0db2311693.png width = "300">


3. Polynomial Kernel:

![image](https://user-images.githubusercontent.com/54764108/165977587-deda2890-61fc-489c-8bf9-3316526903f4.png)

<img src = https://user-images.githubusercontent.com/54764108/165978949-7f5e34c0-7bbb-43b1-bf11-57448f8a9cbf.png width = "300">


### Importing SVC from svm module:

Kernel SVm is a sensible classifer. It greatly tries to prevent overfitting. Because it jumps to a higher dimention to distribute data in such a way so that the data are linearly seperable.

```python
from sklearn.svm import SVC
classifier = SVC(kernel= 'rbf' , random_state= 0)
classifier.fit(X_train,y_train)
```

#### confusion matrix:
```python
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
```
![image](https://user-images.githubusercontent.com/54764108/165981796-9ac3d1e0-1e5a-4b2b-bd75-88aae9a6aa71.png)

Total mispredictions = 4 + 3 = 7

#### Visualizing data:

```python
# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Kernel SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
```
![image](https://user-images.githubusercontent.com/54764108/165981988-cd167529-91dd-4111-a607-57514ce9a893.png)


```python
# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Kernel SVM (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
```
![image](https://user-images.githubusercontent.com/54764108/165982083-3af3e4c9-5ec4-4fd3-b3fb-6d4968bd67be.png)


