# Evaluating Classification Model's Performance

### False Positives and False Negatives
As we know from logistic classifier intuition, <br>
For four random values of the independent variables will end up in terms of y (hat) / in terms of predicted value of dependent variable <br>
And anything below 50% line is considered "false" and "true" if otherwise.
<img src =https://user-images.githubusercontent.com/54764108/167260719-123bb148-e6f8-4e4b-8a97-d1778ad13ea9.png width = "500">


Now considering known results and comparing them with our predicted model:
<img src =https://user-images.githubusercontent.com/54764108/167260940-9b569604-0592-4268-a448-1da693a0c2cb.png width = "500"> <img src =https://user-images.githubusercontent.com/54764108/167261169-fc20476a-0c23-4b8e-bd3d-e91f8234ed0c.png width = "500"> 

So, we can conclude that the logistic regression made an error here <br>
1. Mistakes made at the top are called **"False Positive" (Type 1 error)**
2. Mistakes made at the bottom are called **"False Negative" (Type 2 error)**


* False positives are mild error because we just perdicted that "an event will happen" when in reality it does not. And its comparatively easier to rollback and rectify that mistake (Warning)<br>
* HOwever, False negatives are much worse in the sense that we predict "an event will not happen" when it actually does. And we may already taken measures according to such hypothesis which may not be easy to be rectified.

But both are significant error in terms of mathematical analysis and we simply cant ignore these.


### Confusion Matrix

<img src =https://user-images.githubusercontent.com/54764108/167261656-eb9f1031-6986-4ff7-ab89-afa5b691f32e.png width = "500">
so indices of the matrix define if the model is aligned with the actual result. [0][1] and [1][0] dimentions indicate the errors/mispredictions made by the model

#### Analysis
1. Accuracy rate = Correct / Total <br>
                 = 85 / 100 = 85 %
2. Error rate = Wrong / Total  <br>
              = 15 / 100 = 15 %

### Accuracy Paradox
<img src =https://user-images.githubusercontent.com/54764108/167261922-2ae7c3e7-c957-40f7-916e-f1525b042bdb.png width = "500">

from the given confusion matrix we can have 2 scenarios:
1. Scenario 1: Accuracy Rate = 9,800 / 10,000 = 98 %
2. Scenario 2:
Now considering our model only predicts if the event "DOES NOT OCCUR" <br>
then new confusion matrix:
<img src =https://user-images.githubusercontent.com/54764108/167262030-02025d22-3823-4257-9daf-92a371eb1de3.png width = "500">

Now, Accuracy Rate = 9,850 / 10,000 = 98.5 %

this is why judging by only accuracy rate is not enough to evaluate a model.

### CAP Curve
A cumulative accuracy profile (CAP) is a concept utilized in data science to visualize discrimination power. **The CAP of a model represents the cumulative number of positive outcomes along the y-axis versus the corresponding cumulative number of a classifying parameter along the x-axis.**

<img src =https://user-images.githubusercontent.com/54764108/167268138-e93c3509-4386-436c-b06e-896331568ff4.png width = "500">

> * This red curve is called the CAP curve of your model. <br>
> * The larger the area under the red and above the blue (in between), the better the model is <br>
> * The green CAP curve is found when we vary some factors(i.e., decreasing independent deatures, changing constants, smth went wrong and thus less area, less suitable model) and thus we can compare these CAP curves to evaluate the models.
> * Grey line is made by the perfect predictions(i. e., only considering those who make the event happen (true) and thus find a steep curve. its like a crystal ball.

**CAP =** Cumulative Accuracy Profile <br>
not equals to <br>
**ROC =** Receiver Operating Characteristic


### CAP curve analysis
<img src =https://user-images.githubusercontent.com/54764108/167268562-8674c36b-6b5a-4f46-ac3e-d5c1f734e55f.png width = "500">

* The area between the grey and blue curve is a<sub>P</sub>
* The area between the red and blue curve is a<sub>R</sub>
* AND AR = a<sub>R</sub> / a<sub>P</sub>
* Value is between 0 and 1
* The closer it is to 1 the better

However, it is quite complicated to find these area under the curves. <br>
So, what we can do is to look at the 50 % line of the horizontal axis, project on our model (CAP Curve) and check where that is in our vertical axis. meaning how many positive outcomes does our model generate if it takes 50 % of the data.

<img src =https://user-images.githubusercontent.com/54764108/167268679-937191a4-78b1-42ce-8683-b8f43d8b7606.png width = "500">

And based on the percentage of X (thumb rule) we can conclude about our model:
<pre>
X < 60 %          Rubbish
60 % < X < 70 %   Poor
70 % < X < 80 %   Good
80 % < X < 90 %   Very Good
90 % < X < 100 %  Too Good (might cause overfitting)
</pre>


