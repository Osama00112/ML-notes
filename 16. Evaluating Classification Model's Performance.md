# Evaluating Classification Model's Performance

### False Positives and False Negatives
As we know from logistic classifier intuition, <br>
For four random values of the independent variables will end up in terms of y (hat) / in terms of predicted value of dependent variable <br>
And anything below 50% line is considered "false" and "true" if otherwise.
<img src =https://user-images.githubusercontent.com/54764108/167260719-123bb148-e6f8-4e4b-8a97-d1778ad13ea9.png width = "500">


Now considering known results and comparing them with our predicted model:
<img src =https://user-images.githubusercontent.com/54764108/167260940-9b569604-0592-4268-a448-1da693a0c2cb.png width = "500"> <img src =https://user-images.githubusercontent.com/54764108/167261169-fc20476a-0c23-4b8e-bd3d-e91f8234ed0c.png width = "500"> 

So, we can conclude that the logistic regression made an error here <br>
1. Mistakes made at the top are called **"False Positive" (Type 1 error)**
2. Mistakes made at the bottom are called **"False Negative" (Type 2 error)**


* False positives are mild error because we just perdicted that "an event will happen" when in reality it does not. And its comparatively easier to rollback and rectify that mistake (Warning)<br>
* HOwever, False negatives are much worse in the sense that we predict "an event will not happen" when it actually does. And we may already taken measures according to such hypothesis which may not be easy to be rectified.

But both are significant error in terms of mathematical analysis and we simply cant ignore these.


### Confusion Matrix

<img src =https://user-images.githubusercontent.com/54764108/167261656-eb9f1031-6986-4ff7-ab89-afa5b691f32e.png width = "500">
so indices of the matrix define if the model is aligned with the actual result. [0][1] and [1][0] dimentions indicate the errors/mispredictions made by the model

#### Analysis
1. Accuracy rate = Correct / Total <br>
                 = 85 / 100 = 85 %
2. Error rate = Wrong / Total  <br>
              = 15 / 100 = 15 %

### Accuracy Paradox
<img src =https://user-images.githubusercontent.com/54764108/167261922-2ae7c3e7-c957-40f7-916e-f1525b042bdb.png width = "500">

from the given confusion matrix we can have 2 scenarios:
1. Scenario 1: Accuracy Rate = 9,800 / 10,000 = 98 %
2. Scenario 2:
Now considering our model only predicts if the event "DOES NOT OCCUR" <br>
then new confusion matrix:
<img src =https://user-images.githubusercontent.com/54764108/167262030-02025d22-3823-4257-9daf-92a371eb1de3.png width = "500">

Now, Accuracy Rate = 9,850 / 10,000 = 98.5 %

this is why judging by only accuracy rate is not enough to evaluate a model.

### CAP Curve
A cumulative accuracy profile (CAP) is a concept utilized in data science to visualize discrimination power. **The CAP of a model represents the cumulative number of positive outcomes along the y-axis versus the corresponding cumulative number of a classifying parameter along the x-axis.**

<img src =https://user-images.githubusercontent.com/54764108/167268138-e93c3509-4386-436c-b06e-896331568ff4.png width = "500">

> * This red curve is called the CAP curve of your model. <br>
> * The larger the area under the red and above the blue (in between), the better the model is <br>
> * The green CAP curve is found when we vary some factors(i.e., decreasing independent deatures, changing constants, smth went wrong and thus less area, less suitable model) and thus we can compare these CAP curves to evaluate the models.
> * Grey line is made by the perfect predictions(i. e., only considering those who make the event happen (true) and thus find a steep curve. its like a crystal ball.

**CAP =** Cumulative Accuracy Profile <br>
not equals to <br>
**ROC =** Receiver Operating Characteristic

