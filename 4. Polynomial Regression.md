# Polynomial Regression

```
y = b0 + b1*x^1 + b2*x^2 + ... bn*x^n

x's -> independent var 
y   -> dependent var
```
### Importing polynomial features from sklearn
then we transform x into a poly feature of degree 2. <br>
and so it now depends not only on the position levels but also the polynomial terms of those levels.

```pyhton 
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 2)
X_poly = poly_reg.fit_transform(X)
```

Now that we have our desired polynomials of X, we use a defferent linear regression object to fit X_poly vs y

```python
lin_reg2 = LinearRegression()
lin_reg2.fit(X_poly,y)
```

### Fitting data in our linear model
```python
plt.scatter(X,y, color = 'orange')
plt.plot(X, lin_reg.predict(X), color = 'blue')
plt.title('Truth or bluff(Linear_Regression)')
plt.xlabel('Positions')
plt.ylabel('Salaries')
plt.show()
```
![image](https://user-images.githubusercontent.com/54764108/164754804-e9ebddec-78cf-4991-aae5-7fec52c8a39a.png)

### fitting the data in our polynomial model
here its normal to assume that only replacing "lin_reg" with "lin_reg2" would suffice.<br> <p> However, "lin_reg2" is still our linear regression object(not polynomial). and we still need to transform the x features to polynomial, so we included that inside the method</p>
```python
predict()
``` 
Also we cant simply put "X_poly" because we want newly added X levels to be included as well. <br>
So we wrote
```python
predict(poly_reg.fit_transform(X))
```  


```python
plt.scatter(X,y, color = 'orange')
plt.plot(X, lin_reg2.predict(poly_reg.fit_transform(X)) , color = 'blue')
plt.title('Truth or bluff(Polynomial_Regression)')
plt.xlabel('Positions')
plt.ylabel('Salaries')
plt.show()
```

![image](https://user-images.githubusercontent.com/54764108/164754919-ad54eac9-cf99-4085-8cb7-a84a792dc34a.png)

### Increasing degree to 3

```python
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 3)
X_poly = poly_reg.fit_transform(X)

lin_reg2 = LinearRegression()
lin_reg2.fit(X_poly,y)

plt.scatter(X,y, color = 'orange')
plt.plot(X, lin_reg2.predict(poly_reg.fit_transform(X)) , color = 'blue')
plt.title('Truth or bluff(Polynomial_Regression)')
plt.xlabel('Positions')
plt.ylabel('Salaries')
plt.show()
```
![image](https://user-images.githubusercontent.com/54764108/164755100-9e32fd5f-f96e-4363-9437-1875c35661b8.png)

### Degree = 4: (more accurate)
![image](https://user-images.githubusercontent.com/54764108/164755214-2713e012-f54e-4618-a224-99610b445db1.png)

### To make the graph more smooth, we can scale the X coordinate to make it more precise... (continuous) 

```python
X_grid = np.arange(min(X),max(X), 0.1)
X_grid = X_grid.reshape(len(X_grid),1)
```
#### plot:
```python
plt.scatter(X,y, color = 'orange')
plt.plot(X_grid, lin_reg2.predict(poly_reg.fit_transform(X_grid)) , color = 'blue')
plt.title('Truth or bluff(Polynomial_Regression)')
plt.xlabel('Positions')
plt.ylabel('Salaries')
plt.show()
```
![image](https://user-images.githubusercontent.com/54764108/164755488-a272e2e8-5c05-4661-b237-eeb512ea53e2.png)

### Predicting results based on our models (linear and polynomial)
```python
lin_reg.predict([[6.5]])
```
> output: array([330378.78787879])

```python
lin_reg2.predict(poly_reg.fit_transform([[6.5]]))
```
> output: 58862.45265155])
